remove(list = ls())

setwd("/home/marlon/mainfolder/marlon/USFQ/DataMining/6_DecisionTrees/P6")

data = readRDS("SPECTF_FINAL.rds")
data$OVERALL_DIAGNOSIS = as.factor(data$OVERALL_DIAGNOSIS)

library(vtreat)
library(pROC)

k = 10

splitPlan = kWayCrossValidation(nrow(data), k, NULL, NULL)

#ID3

source("ID3.R")

data2 = data[c(2,3,4,5,1)]

bins = c(-Inf,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, Inf)

data2$F3S = cut(data2$F3S, breaks = bins)
data2$F13S = cut(data2$F13S, breaks = bins)
data2$F20S = cut(data2$F20S, breaks = bins)
data2$F21R = cut(data2$F21R, breaks = bins)

recall3 = c()
accuracy3 = c()
precision3 = c()
auc3 = c()
pred_vector3 = rep(0, nrow(data))

for(i in 1:k){
    split = splitPlan[[i]]
    
    model = ID3(data2[split$train, ], "OVERALL_DIAGNOSIS")
    
    pred_vector3[split$app] = predict_ID3(data2[split$app, ], model)
    
    pred = predict_ID3(data2[split$app, ], model)
    
    real = data2[split$app, ]$OVERALL_DIAGNOSIS
    
    confusion = as.matrix(table(pred, real))
    
    recall3[i] = confusion[2,2] / (confusion[1,2] + confusion[2,2])
    accuracy3[i] = (confusion[1,1] + confusion[2,2]) / sum(confusion)
    precision3[i] = confusion[2,2] / (confusion[2,2] + confusion[2,1])
    x = roc(as.numeric(real), (as.numeric(pred) - 1))
    auc3[i] = x$auc
}

pred_vector3 = as.numeric(pred_vector3)
real_vector = data$OVERALL_DIAGNOSIS

#Matriz de confusion
table(pred_vector3, real_vector)


#Promedios y Desviaciones
sprintf("Accuracy   Mean: %f   SD: %f", mean(accuracy3), sd(accuracy3))
sprintf("Precision   Mean: %f   SD: %f", mean(precision3), sd(precision3))
sprintf("Recall   Mean: %f   SD: %f", mean(recall3), sd(recall3))
sprintf("AUC   Mean: %f   SD: %f", mean(auc3), sd(auc3))

library(precrec)
#Plot AUC vs #Plot presicion vs recall
precrec_obj3 <- evalmod(scores = pred_vector3, labels = real_vector)
plot(precrec_obj3)
precrec_obj3


library(rpart)

fmla = as.formula("OVERALL_DIAGNOSIS ~ .")

#GINI

recall2 = c()
accuracy2 = c()
precision2 = c()
auc2 = c()
pred_vector2 = rep(0, nrow(data))

for(i in 1:k){
    split = splitPlan[[i]]

    model = rpart(fmla, data = data[split$train, ], method = "class")

    pred_vector2[split$app] = predict(model, newdata = data[split$app, ], type = "class")

    pred = predict(model, newdata = data[split$app, ], type = "class")

    real = data[split$app, ]$OVERALL_DIAGNOSIS

    confusion = as.matrix(table(pred, real))

    recall2[i] = confusion[2,2] / (confusion[1,2] + confusion[2,2])
    accuracy2[i] = (confusion[1,1] + confusion[2,2]) / sum(confusion)
    precision2[i] = confusion[2,2] / (confusion[2,2] + confusion[2,1])
    x = roc(as.numeric(real), (as.numeric(pred) - 1))
    auc2[i] = x$auc
}

pred_vector2 = pred_vector2 - 1
real_vector = data$OVERALL_DIAGNOSIS

#Matriz de confusion
table(pred_vector2, real_vector)

#Promedios y Desviaciones
sprintf("Accuracy   Mean: %f   SD: %f", mean(accuracy2), sd(accuracy2))
sprintf("Precision   Mean: %f   SD: %f", mean(precision2), sd(precision2))
sprintf("Recall   Mean: %f   SD: %f", mean(recall2), sd(recall2))
sprintf("AUC   Mean: %f   SD: %f", mean(auc2), sd(auc2))

#Plot AUC y Plot presicion vs recall
library(precrec)
precrec_obj2 <- evalmod(scores = pred_vector2, labels = real_vector)
plot(precrec_obj2)
precrec_obj2

#GAIN RATIO
library(RWeka)

recall = c()
accuracy = c()
precision = c()
auc = c()
pred_vector = rep(0, nrow(data))

for(i in 1:k){
    split = splitPlan[[i]]

    model = J48(fmla, data = data[split$train, ])

    pred_vector[split$app] = predict(model, newdata = data[split$app, ], type = "class")

    pred = predict(model, newdata = data[split$app, ], type = "class")

    real = data[split$app, ]$OVERALL_DIAGNOSIS

    confusion = as.matrix(table(pred, real))

    recall[i] = confusion[2,2] / (confusion[1,2] + confusion[2,2])
    accuracy[i] = (confusion[1,1] + confusion[2,2]) / sum(confusion)
    precision[i] = confusion[2,2] / (confusion[2,2] + confusion[2,1])
    x = roc(as.numeric(real), (as.numeric(pred) - 1))
    auc[i] = x$auc
}

pred_vector = pred_vector - 1


#Matriz de confusion
table(pred_vector, real_vector)

#Promedios y Desviaciones



sprintf("Accuracy   Mean: %f   SD: %f", mean(accuracy), sd(accuracy))
sprintf("Precision   Mean: %f   SD: %f", mean(precision), sd(precision))
sprintf("Recall   Mean: %f   SD: %f", mean(recall), sd(recall))
sprintf("AUC   Mean: %f   SD: %f", mean(auc), sd(auc))

#Plot AUC y #Plot presicion vs recall
precrec_obj <- evalmod(scores = pred_vector, labels = real_vector)
plot(precrec_obj)
precrec_obj

